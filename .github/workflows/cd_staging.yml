name: CD to Staging

on:
  # push:
  #   tags:
  #     - 'v*' # Push events to matching v*, i.e. v1.0, v20.15.10
  pull_request:
    branches:  
      - 'main'         


jobs:
  cd-staging:

    runs-on: ubuntu-latest
    strategy:
      max-parallel: 4

    env:
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }} 
      DATABRICKS_TOKEN:  ${{ secrets.DATABRICKS_TOKEN }}
      # DATABRICKS_JOBS_API_VERSION: 2.1

    steps:
      # - uses: actions/checkout@v1
      - uses: actions/checkout@v2
        with:
          fetch-depth: 0  # Shallow clones should be disabled for a better relevancy of analysis   

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.7.9

      - name: Install packages
        run: |
          make install


      # 1. Deploy & run integration tests
      # - name: Deploy integration test
      #   run: |
      #     dbx deploy --jobs=cicd-databricks-github-sample-integration-test --files-only

      # - name: Run integration test
      #   run: |
      #     dbx launch --job=cicd-databricks-github-sample-integration-test --as-run-submit --trace    



      # 2. Deploy & run training pipeline
      # - name: Deploy the training job (as one-time job)
      #   run: |
      #     dbx deploy --jobs=ci-train-job --files-only
       
      # - name: Run the training job (as one-time job)
      #   run: |
      #     dbx launch --job=ci-train-job --as-run-submit --trace   

      # Add in case we want to deploy the job and have it present in the Jobs UI, scheduled
      - name: Deploy the training job (as retraining pipeline job)
        run: |
          dbx deploy --jobs=ci-train-job  

      - name: Run the training job (as one-time job)
        run: |
          dbx launch --job=ci-train-job --trace


      # # HERE THERE SHOULD BE A IF TEST: IF PERFORMANCE IS ACCEPTABLE, DO NEXT STEPS... 

      # # REGISTER THE MODEL TO MLFLOW ()
          

      # # 4. Deploy the Inference Job to Staging 
      # # 4a. Deploy just the job (with native scheduling) to have it triggered by Databricks' native Job scheduler
      # - name: Deploy the inference job
      #   run: |
      #     dbx deploy --jobs=cd-infer-job-scheduled-staging          

      # 4b. Deploy just the job (no scheduling) to have it triggered by AWS Managed Airflow
      - name: Deploy the inference job
        run: |
          dbx deploy --jobs=cd-infer-job-staging 

      - name: Run the training job (as one-time job)
        run: |
          dbx launch --job=cd-infer-job-staging --trace   



# rubbish

     # TODO: deploy files to workspace
      # - name: Deploy notebooks
      #   shell: bash
      #   run: |
      #     echo "Deploying notebooks"
      #     databricks workspace import_dir --overwrite notebooks/ /Released/notebook/ 
       
